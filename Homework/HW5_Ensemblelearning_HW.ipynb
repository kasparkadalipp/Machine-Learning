{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0tnOpaLBcqT"
      },
      "source": [
        "# Homework #5\n",
        "\n",
        "## Ensemble learning\n",
        "\n",
        "This colaboratory contains Homework #5 of the Machine Learning course, which is due **November 13, midnight (23:59 EEST time)**. To complete the homework, extract **(File -> Download .ipynb)** and submit to the course webpage.\n",
        "\n",
        "\n",
        "## Submission's rules:\n",
        "\n",
        "1.   Please, submit only .ipynb that you extract from the Colaboratory.\n",
        "2. Run your homework exercises before submitting (output should be present, preferably restart the kernel and press run all the cells).\n",
        "3. Do not change the description of tasks in red (even if there is a typo|mistake|etc).\n",
        "4. Please, make sure to avoid unnecessary long printouts.\n",
        "5. Each task should be solved right under the question of the task and not elsewhere.\n",
        "6. Solutions to both regular and bonus exercises should be submitted in one IPYNB file.\n",
        "\n",
        "Please, steer clear of copying someone else's work. If you discuss assignments with anyone in the course, please, mention their names here:\n",
        "\n",
        "Pooh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69QmUc_2rfZT"
      },
      "source": [
        "##List of Homework's exercises:\n",
        "\n",
        "1.   [Ex1](#scrollTo=ux5PBYkbwewj) - 3 points\n",
        "2.   [Ex2](#scrollTo=Gezm0AO80ary) - 4 points\n",
        "3.   [Ex3](#scrollTo=avCryKaDzJqn) - 3 points\n",
        "4.   [Bonus 1](#scrollTo=jdZkblZW7bEp) - 2 points (based on quality of presentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wptpa5-JBUbu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For plotting like a pro\n",
        "!pip install -q plotnine\n",
        "from plotnine import *\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXaRp8z7tmbD"
      },
      "source": [
        "def create_random_2c_data (D, N):\n",
        "  \"\"\"\n",
        "  Function create_random_2c_data generates two sets of D dimensional \n",
        "  points (N points each), one for each class. The first set is sampled from D \n",
        "  dimensional Gaussian distribution with mean 0 and standard deviation 1. The \n",
        "  second set is generated from the distribution, with mean 1 and standard \n",
        "  deviation 1.\n",
        "  \"\"\"\n",
        "  # Generating N points for the first class\n",
        "  mu_vec1 = np.zeros(D) # creates a vector of zeros, these are averages across each dimension\n",
        "  cov_mat1 = np.eye(D) # creates a diagonal matrix of size D x D, all values except diagonal are 0\n",
        "  class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, N)\n",
        "\n",
        "  # The same stuff as above, just averages are shifted into 1\n",
        "  mu_vec2 = np.ones(D) # creates a vector of ones\n",
        "  cov_mat2 = np.eye(D)\n",
        "  class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, N)\n",
        "\n",
        "  # a lot of boring things....\n",
        "  # gluing together two matrices generated above\n",
        "  data = pd.DataFrame(np.concatenate((class1_sample, class2_sample)))\n",
        "\n",
        "  # Create names for columns\n",
        "  data.columns = [ 'x' + str(i) for i in (np.arange(D)+1)]\n",
        "\n",
        "  # Create a class column\n",
        "  data['class'] = np.concatenate((np.repeat(0, N), np.repeat(1, N)))\n",
        "\n",
        "  # This is important for plotting and modelling\n",
        "  data['class'] = data['class'].astype('category')\n",
        "\n",
        "  return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux5PBYkbwewj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Homework exercise 1: implement and train a bagging classifier with 3 K-NN models as estimators (3 points)\n",
        "\n",
        "\n",
        "<font color='red'> In this exercise you will need to use `classify_knn` function from the first practice session to train three different KNN models on three resamples of this dataset. </font> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTaUH_073gW7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "np.random.seed(2342347823) # random seed, this number was random, no need to make conspiracies around it\n",
        "\n",
        "D = 2 # two dimensions\n",
        "N = 100 # points per class\n",
        "\n",
        "whole_data = create_random_2c_data(D, N)\n",
        "\n",
        "# Randomly splitting data into train (60%) and validation (40%)\n",
        "train, val = train_test_split(whole_data, random_state = 111, test_size = 0.40) \n",
        "\n",
        "n_bootstraps = 3\n",
        "np.random.seed(1111)\n",
        "\n",
        "# creating resamples\n",
        "resamples = [resample(train, n_samples = int(len(train)*0.8), replace=False).index.values for i in range(n_bootstraps)]\n",
        "\n",
        "# first resample\n",
        "train_resample1 = train.loc[resamples[0]]\n",
        "\n",
        "# second resample\n",
        "train_resample2 = train.loc[resamples[1]]\n",
        "\n",
        "# third resample\n",
        "train_resample3 = train.loc[resamples[2]]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312O_UNcJI8l"
      },
      "source": [
        "<font color='red'> Here, I just convert pandas DataFrame into Numpy arrays that are easier to use list comprehension mechanisms on. </font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q0MBD_RBQPh"
      },
      "source": [
        "train1 = np.asarray(train_resample1[['x1','x2']])\n",
        "labels1 = np.asarray(train_resample1[['class']]).reshape((train_resample1.shape[0]))\n",
        "\n",
        "train2 = np.asarray(train_resample2[['x1','x2']])\n",
        "labels2 = np.asarray(train_resample2[['class']]).reshape((train_resample2.shape[0]))\n",
        "\n",
        "train3 = np.asarray(train_resample3[['x1','x2']])\n",
        "labels3 = np.asarray(train_resample3[['class']]).reshape((train_resample3.shape[0]))\n",
        "\n",
        "val_points = np.asarray(val[['x1','x2']])\n",
        "val_labels = np.asarray(val[['class']]).reshape((val.shape[0]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDGtTjHrIaGS"
      },
      "source": [
        "<font color='red'>  **(Homework exercise 1- a)** Copy and adapt `classify_knn` function from the first homework and practice session to operate on 2D points. **(1 point)**</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1thhyeB9wlAb"
      },
      "source": [
        "def dist(point1, point2): # function dist is also from the first practice session\n",
        "  # sum of squared coordinate-wise differences under sqrt\n",
        "  return(np.sqrt(np.sum((point2 - point1)**2)))\n",
        "\n",
        "def classify_knn(val_point, k, train, labels):\n",
        "  ##### YOUR CODE STARTS #####\n",
        "  all_distances = [dist(val_point, point) for point in train]\n",
        "  nearest_neighbours = np.argsort(all_distances)\n",
        "  predicted_classes = [labels[index] for index in nearest_neighbours[:k]]\n",
        "  prediction = max(predicted_classes, key=predicted_classes.count)\n",
        "  ##### YOUR CODE ENDS ##### \n",
        "  \n",
        "  return prediction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLrqeWyJKPTi"
      },
      "source": [
        "<font color='red'> Test that the function was adapted correctly by running the following example </font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4zB75roKPv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef5a799-2bc4-481d-b4b5-84617cfca545"
      },
      "source": [
        "val_point = val_points[1]\n",
        "print(f'predicted class of the first point is {classify_knn(val_point, 5,  train1, labels1)}, while the true class is {val_labels[1]}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted class of the first point is 0, while the true class is 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNP4AnDoxa5v"
      },
      "source": [
        "<font color='red'> **(Homework exercise 1- b)** Classify each point from the validation set using `classify_knn` function. Use different resamples and list comprehension [(do something with point) for point in points]. Fix `k` at 5. **(1 point)**</font> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Rzlx-DydQj"
      },
      "source": [
        "k = 5\n",
        "\n",
        "# Use three K-NN models that work on three different resamples\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "val['knn1'] = [classify_knn(val_point, k, train1, labels1) for val_point in val_points]\n",
        "val['knn2'] = [classify_knn(val_point, k, train2, labels2) for val_point in val_points]\n",
        "val['knn3'] = [classify_knn(val_point, k, train3, labels3) for val_point in val_points]\n",
        "##### YOUR CODE ENDS ##### "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qunbkwu6MID6"
      },
      "source": [
        "<font color='red'> **(Homework exercise 1- c)** Aggregate individual predictions using the majority vote approach **(0.5 points)**</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRVy2QZ9zDvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f224b93-3351-431a-de8a-b2c5740862f7"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "val['knn_bagging'] = val[['knn1', 'knn2', 'knn3']].mode(axis = 1)\n",
        "##### YOUR CODE ENDS ##### \n",
        "\n",
        "print(f\"Accuracy of hand made bagged ensemble with 3 KNNs is {np.sum(val['knn_bagging'] == val['class'])/len(val[['class']])*100}%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of hand made bagged ensemble with 3 KNNs is 75.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-LNr7etxhUA"
      },
      "source": [
        "<font color='red'> **(Homework exercise 1- d)** Use sklearn `BaggingClassifier` to implement analogous model that uses KNeighborsClassifier as an estimator (with k = 5). Don't forget to use a random state for reproducibility.\n",
        "\n",
        "Assess its performance on the same validation set and display it. **(0.5 points)**</font> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl2Y6P1jE7k0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6867a6cc-966c-4135-8362-ddce53737cb8"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "knn_begger = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=k), max_samples=0.8, n_estimators=3, random_state=0).fit(train[['x1','x2']], train[['class']])\n",
        "##### YOUR CODE ENDS ##### \n",
        "print(f\"Accuracy of sklearn bagging with {3} KNNs {knn_begger.score(val[['x1', 'x2']], val[['class']])*100}%\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of sklearn bagging with 3 KNNs 70.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gezm0AO80ary"
      },
      "source": [
        "## Homework exercise 2: eXtreme Gradient Boosting (XGBoost) (4 points)\n",
        "\n",
        "<font color='red'> Let's finally build for ourselves a new shiny XGBoost model, the most popular algorithm for Kaggle competitions. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gauRESFyT03k"
      },
      "source": [
        "<font color='red'> First, we need to load data (we shall use MNIST data again). </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ZTcrSLPFkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4885ac78-5d9a-43df-baa2-f35b0b5688f9"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(images, labels) = mnist.load_data()[0]\n",
        "\n",
        "# reshape into a matrix format\n",
        "images = images.reshape(-1, 28*28)\n",
        "\n",
        "# use fewer images for faster training\n",
        "train_images = images[0:2000]\n",
        "train_labels = labels[0:2000]\n",
        "\n",
        "test_images = images[2000:3000]\n",
        "test_labels = labels[2000:3000]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKc_JzT5UN0n"
      },
      "source": [
        "<font color='red'> **(Homework exercise 2- a)** Use the tutorial page (https://xgboost.readthedocs.io/en/latest/python/python_intro.html and https://www.kaggle.com/anktplwl91/mnist-xgboost) to fill in the gaps in the following code and traing the XGBoost model. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzsOrKP0ENjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7e26f4-c001-4d6d-bc01-f35ee1f4baa6"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "\n",
        "# XGBoosts wants data to be wrapped into special formats\n",
        "dtrain = xgb.DMatrix(train_images, label=train_labels)\n",
        "dtest = xgb.DMatrix(test_images, label=test_labels)\n",
        "\n",
        "# most meaningful parameters\n",
        "param_list = [(\"objective\", \"multi:softmax\"), (\"eval_metric\", \"merror\"), (\"num_class\", 10)]\n",
        "\n",
        "# Number of trees\n",
        "n_rounds = 600\n",
        "\n",
        "# if nothing seems to improve for 50 iterations - stop\n",
        "early_stopping = 50\n",
        "\n",
        "# train for training and test for ... validation!    \n",
        "eval_list = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "\n",
        "# 1,2,3.. go!\n",
        "\n",
        "%time bst = xgb.train(param_list, dtrain, n_rounds, eval_list, verbose_eval=False)\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 21s, sys: 488 ms, total: 4min 21s\n",
            "Wall time: 2min 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27j0swxUi4D"
      },
      "source": [
        "<font color='red'> **(Homework exercise 2- b)** Use the same tutorial page (https://xgboost.readthedocs.io/en/latest/python/python_intro.html) to find out how to evaluate the model **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8fHF6FtRJJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468531f4-f499-4525-e111-eb693d0ac34b"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_images_2 = images[3000:4000]\n",
        "test_labels_2 = labels[3000:4000]\n",
        "\n",
        "ypred = bst.predict(xgb.DMatrix(test_images_2))\n",
        "print(\"XGBoost accuracy:\", accuracy_score(ypred, test_labels_2))\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost accuracy: 0.911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dQ4Q2EnU7ur"
      },
      "source": [
        "<font color='red'> Are you impressed with XGBoost performance? </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK3GFrVcVAcA"
      },
      "source": [
        "<font color='red'> **(Homework exercise 2- c)** Train Adaptive Boosting, Gradient Boosting and a simple KNN model from sklearn (KNeighborsClassifier) on the same trainign data and evaluate on the same test data. For each model use the default hyperparameters (e.g. `n_estimators` or `n_neighbors`). If you do not want to use default parameters, you can use `cross_val_score` to pick the best values for the hyperparameters using training data. Compare the performance of these three models and XGBoost and draw conclusions in a separate text cell.  **(2 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VLD9VUkupuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d420a8-0b74-4a9e-b955-301cf40d67cc"
      },
      "source": [
        "# AdaBoostClassifier\n",
        "%%time\n",
        "##### YOUR CODE STARTS #####\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "abc = AdaBoostClassifier()\n",
        "abc.fit(train_images, train_labels)\n",
        "print(\"Accuracy:\", abc.score(test_images, test_labels))\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.524\n",
            "CPU times: user 2.85 s, sys: 19.9 ms, total: 2.87 s\n",
            "Wall time: 2.92 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTNmcXQ8vN9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e0da31-bb1c-4b99-9e97-bef90f1348d5"
      },
      "source": [
        "# GradientBoostingClassifier\n",
        "%%time\n",
        "# might take considerable time if trained with default number of n_estimators\n",
        "##### YOUR CODE STARTS #####\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(train_images, train_labels)\n",
        "print(\"Accuracy:\", gbc.score(test_images, test_labels))\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.891\n",
            "CPU times: user 1min 42s, sys: 94.7 ms, total: 1min 42s\n",
            "Wall time: 1min 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5yjj2PsTsLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b362426b-7b9c-4b67-9e8e-05d8fd2832ca"
      },
      "source": [
        "# KNeighborsClassifier\n",
        "%%time\n",
        "##### YOUR CODE STARTS #####\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier()\n",
        "neigh.fit(train_images, train_labels)\n",
        "print(\"Accuracy:\", neigh.score(test_images, test_labels))\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.912\n",
            "CPU times: user 303 ms, sys: 87.6 ms, total: 391 ms\n",
            "Wall time: 207 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5PHU3XgVR2C"
      },
      "source": [
        "<font color='red'> How these models compare with each other and to XGBoost? Can you try to elaborate on this difference? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FfQTIPYAyG"
      },
      "source": [
        "# Write your comment here:\n",
        "\n",
        "# XGBoost and GradinetBoosting are sequential so they take a lot longer to train.\n",
        "# AdaBoost recieved a rather low accuracy, remaining models achieved very similar accuracies.\n",
        "# Changing AdaBoost default estimator to SVC(probability=True, kernel='linear') got an accuracy of 0.911.\n",
        "# So its accuracy must be low because of the suboptimal base estimator."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCryKaDzJqn"
      },
      "source": [
        "## Homework exercise 3: implement blending approach (3 points)\n",
        "<font color='red'> In this exercise you will practice using blending approach to meta-learning. </font>\n",
        "\n",
        "<font color='red'> **(Homework exercise 3- a)** to implement blending we first need to create a separate validation set that would be independent from training and test data. Below, use images from 0 to 1500 as training data, images from 1500 to 2000 as validation and from 2000 to 3000 as a test set. **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLaLzggzOAX"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(images, labels) = mnist.load_data()[0]\n",
        "\n",
        "# reshape into a matrix format\n",
        "images = images.reshape(-1, 28*28)\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "train_images = images[0:1500]\n",
        "train_labels = labels[0:1500]\n",
        "\n",
        "val_images = images[1500:2000]\n",
        "val_labels = labels[1500:2000]\n",
        "\n",
        "test_images = images[2000:3000]\n",
        "test_labels = labels[2000:3000]\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58hA8_x7_Ne5"
      },
      "source": [
        "<font color='red'> **(Homework exercise 3- b)** Train three models (decision tree, k nearest neighbors classifier, and the logistic regression) with default parameters on the train data. **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9XoXdKazZac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817aad6a-79b8-41c1-aaa0-7c1d96e3b8b0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = KNeighborsClassifier()\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model1.fit(train_images, train_labels)\n",
        "print(\"LogisticRegression accuracy:\", model1.score(val_images, val_labels))\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model2.fit(train_images, train_labels)\n",
        "print(\"DecisionTreeClassifier accuracy:\", model2.score(val_images, val_labels))\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model3.fit(train_images, train_labels)\n",
        "print(\"KNeighborsClassifier accuracy:\", model3.score(val_images, val_labels))\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression accuracy: 0.87\n",
            "DecisionTreeClassifier accuracy: 0.726\n",
            "KNeighborsClassifier accuracy: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0TGEr4_i6-"
      },
      "source": [
        "<font color='red'> **(Homework exercise 3- c)** Create a training set for the meta-learner by concatenating the predictions made by individual models on validation images. Hint: use function `np.concatenate` and `predict_proba` as we did for stacking. **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E2CMbbnzhxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10940f21-0d80-4ddf-8744-0c4059f7bdb5"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "train_blending =  np.concatenate([model1.predict_proba(val_images),\n",
        "                                 model2.predict_proba(val_images), \n",
        "                                 model3.predict_proba(val_images)], \n",
        "                                axis = 1)\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "train_blending_labels = val_labels\n",
        "train_blending.shape # if all was done correctly this shape should be (500, 30)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNv2gKx4AJ08"
      },
      "source": [
        "<font color='red'> **(Homework exercise 3- d)** Create a test set for the meta-learner by concatenating the predictions made by each model on test images. Use the same function as in the cell above. **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwpZ-Z8oz1xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9124402-fb91-448d-fe1e-966290e7d58e"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "test_blending = np.concatenate([model1.predict_proba(test_images),\n",
        "                                 model2.predict_proba(test_images), \n",
        "                                 model3.predict_proba(test_images)], \n",
        "                                axis = 1)\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "test_blending.shape # if all was done correctly this shape should be (1000, 30)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlIlOIJ_AkoS"
      },
      "source": [
        "<font color='red'> **(Homework exercise 3- e)** Use a new model (SVM) as a meta-learner and train it on the `train_blending` data. **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8MwXQrzujZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537455f7-b95f-48fa-a030-ea0f70a6e234"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "np.random.seed(1111) \n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "blending_model = SVC()\n",
        "blending_model.fit(train_blending, val_labels)\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3lreeTBrPz"
      },
      "source": [
        "<font color='red'> **(Homework exercise 3- f)** Evaluate the performance of the blending ensemble on the test set and comment on the difference between blending and stacking (that we tried in the practice session). Which one would you prefer and why?  **(0.5 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fmgRVmc0Bsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2cab4d-71f0-4cec-9f54-fed9e39e9b65"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "print(f\"Blending esemble accuracy {blending_model.score(test_blending, test_labels)*100}%\")\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending esemble accuracy 89.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0WiXv1WCKnq"
      },
      "source": [
        "# What is your take on the difference between blending and stacking (from practice session)?\n",
        "# Which one would you prefer and why?\n",
        "# Comment here:\n",
        "\n",
        "# Stacking outperformed blending in this example. \n",
        "# Blending uses a validation set and its predictions to build a new model. \n",
        "# Stacking uses folds of training set to do the same. \n",
        "# They are both very similar approaches but I think I would initially prefer blending, \n",
        "# because blending trains a model on only a small protion of the training set and should \n",
        "# therefore be faster than stacking while still getting similarly high accuracy."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNMjKDJWG6Wn"
      },
      "source": [
        "# Bonus exercises\n",
        "*(NB, these are optional exercises!)*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZkblZW7bEp"
      },
      "source": [
        "## Bonus exercise 1 (2 bonus points):\n",
        "<font color='red'> We have seen that in general increasing the number of estimators in an ensemble leads to better performance. In this exercise, you will experiment with the number of estimators in different ensemblers to explore **convergence behavior** and **overfitting**.  \n",
        "You will compare the performance of **bagging of decision trees, random forests, extreme RF, boosting with decision trees and stacking different decision trees.** \n",
        "</font>\n",
        "<font color='red'>\n",
        "* Use MNIST dataset\n",
        "* train different ensembles with various number of decision trees (ranging from 1 to 2000. i.e. choose a small max_depth for speed) \n",
        "* plot the **classification error** with the number of decision trees in each ensembler in the same plot.\n",
        "* Compare the convergence behavior to other ensembles (e.g. RFs as base classifiers)\n",
        "* Explain the behaviour that your observe.\n",
        "</font>\n",
        "\n",
        "<font color='red'>\n",
        "As usual, technical depth and good presentation are the key to get bonus points.\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqmMEkdx8Ptl"
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "\n",
        "##### YOUR CODE ENDS ##### (please do not delete this line)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSswjqM0hlSi"
      },
      "source": [
        "# Comments (optional feedback to the course instructors)\n",
        "Here, please, leave your comments regarding the practice session, possibly answering the following questions: \n",
        "* what would you suggest to add or remove?\n",
        "* anything else you would like to tell us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4shX4mMmDFI"
      },
      "source": [
        "# Add your comments here:\n"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}